{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de6f9cc4-46f6-47c8-8680-e3982fee4474",
   "metadata": {},
   "source": [
    "# Examen 2\n",
    "## Cours A59 - apprentissage par renforcement\n",
    "## \n",
    "-----\n",
    "\n",
    "\r",
    "**Objectif**\n",
    "\n",
    "Nous allons utiliser un réseau DQN mais sur un environnement déjà résolu avec des algorithmes classiques.<br>\n",
    "\n",
    "C'est l'opportunité de tester quelques hypothèses sans pénaliser ceux d'entre vous qui ont des machines moins puissantes.<br>\n",
    "\n",
    "Vous aurez besoin des bibliothèques suivantes :<br>\n",
    " Python 3.11<br>\n",
    " tensorflow 2.17<br>\n",
    " gymnasium <= 1.0<br>\n",
    " numpy<br>\n",
    " matplotlib<br>\n",
    "\n",
    "Deux fichiers sont diponibles pour installer des environnements virtuels compatibles (requirementsxx.txt).\n",
    "\n",
    "Les documents sont autorisés.<br>\n",
    "Il s'agit d'un travail personnel.<br>\n",
    "\n",
    "La durée est de 2h30.<br>\n",
    "\n",
    "Le total fait 25 points.<br>\n",
    "Décomposition :<br>\n",
    "Compréhension de l'environnement (3 pts)<br>\n",
    "Réseaux de neurones (4 pts)<br>\n",
    "DQN variantes(3 pts)<br>\n",
    "Fonction de représentation de l'état (3 pts)<br>\n",
    "Exécution et vérification (4 pts)<br>\n",
    "DQN vs Q Learning (3 pts)<br>\n",
    "DQN vs Policy gradient (5pts)<br>\n",
    "\n",
    "### Une fois terminé, merci de m'envoyer le notebook renommé avec votre prénom nom avant l'heure limite.<br>\n",
    "\n",
    "(c) Fabrice Mulotti\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c3f128-35cd-4058-8c0c-238d0efd1e66",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>A faire</b> <br>\n",
    "Vous devez compléter là ou est inscrit<br>\n",
    "    #VOTRE RESPONSE ou <br>\n",
    "    #VOTRE CODE\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ad6d84-2b6e-4627-a263-7dc57529a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ddb3de-19da-4e49-a52e-2e7bd972f225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "print(f\"Tensorflow version {tf.__version__}\")\n",
    "%matplotlib inline\n",
    "matplotlib\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "\n",
    "\n",
    "import time\n",
    "# si vous utilisez keras 2.x, décommentez\n",
    "# tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a7419a-fda1-4a7d-9bdb-03f193d88644",
   "metadata": {},
   "source": [
    "---\n",
    "## Définition de l'environnement\n",
    "\n",
    "Lien vers les informations sur l'environnement : __[Frozen lake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eede1d-4dd6-498f-b2c3-2ea15a288abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"FrozenLake-v1\"\n",
    "\n",
    "env = gym.make(ENV_NAME,is_slippery=False)\n",
    "np.random.seed(100)\n",
    "env.reset(seed=100)\n",
    "nb_actions = env.action_space.n # Nombre d'action\n",
    "nb_obs = env.observation_space.n # nombre de position dans notre environnement\n",
    "print(f\"Nombre d actions : {nb_actions}, Dimension de l env : {nb_obs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc064f29-c6b1-456b-86eb-4d43106ae655",
   "metadata": {},
   "source": [
    "### Remarques sur l'environnement\n",
    "\n",
    "4 actions possibles :<br>\n",
    "    Gauche, bas, droite, haut\n",
    "    \n",
    "Observation :<br>\n",
    "    Valeurs discrètes (0 à 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24623509-4236-47b2-ab89-0aa72da7cc09",
   "metadata": {},
   "source": [
    "## Compréhension de l'environnement (3 pts)\n",
    " la suite du TP il va être important de bien comprendre le principe des récompenses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c45e0ff-17a9-488f-8107-d7ab114375e6",
   "metadata": {},
   "source": [
    "La transition vers **l'objectif** rapporte combien ?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca2653b4-ba28-47f0-864e-082abcca3696",
   "metadata": {},
   "source": [
    "# VOTRE REPONSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf14ba0-91b3-40b4-a90f-ccd21e94dbb0",
   "metadata": {},
   "source": [
    "La transition vers **un puit** rapporte combien ?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "27e2a808-2c58-49f7-96aa-ba495dbf7f8f",
   "metadata": {},
   "source": [
    "# VOTRE REPONSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87af8bd2-2199-4458-868a-aa459e7ba896",
   "metadata": {},
   "source": [
    "La transition vers une **case gelée** (c'est à dire toutes les autres cases) rapporte combien ?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f363486-fd52-40bb-ba1e-8b4d443de61c",
   "metadata": {},
   "source": [
    "# VOTRE REPONSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c81638d-aa42-4a0d-88c4-019ed666a835",
   "metadata": {},
   "source": [
    "Donc un épisode avec echec aura une récompense **totale** (G avec gamma=1) de combien ?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f89e17b7-2416-4cd8-b301-774950dee637",
   "metadata": {},
   "source": [
    "# VOTRE REPONSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77affa37-17c6-426a-8056-dfb72ed85582",
   "metadata": {},
   "source": [
    "Et donc un épisode réussi aura une récompense **totale** (G avec gamma=1) de combien ?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6f5470c-6677-4e9c-9f60-1733356b81ce",
   "metadata": {},
   "source": [
    "# VOTRE REPONSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6056df-4b02-4ea3-8aae-e84cd825869a",
   "metadata": {},
   "source": [
    "Quelles sont donc les deux seules récompenses possibles cumulées pour un épisode ?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5133cb0e-d984-4f44-9261-15970069dcf2",
   "metadata": {},
   "source": [
    "# VOTRE REPONSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab745a3-2eae-461a-9be7-e2ae06c4b044",
   "metadata": {},
   "source": [
    "---\n",
    "## Représentation de l'environnement\n",
    "\n",
    "Nous sommes sur une matrice 4x4.\n",
    "\n",
    "Il existe plusieurs facons de représentation chaque état.\n",
    "\n",
    "Cela pourrait être par exemple, une seule valeur d'entrée, un entier entre 0 et 15.\n",
    "\n",
    "Nous allons opter pour une approche qui montre plus d'efficacité, nous allons représenter le statut par un numpy de 1 dimension de grandeur 16.\n",
    "Toutes les valeurs seront à zéro, sauf l'indice correspondant à la position du joueur qui sera à 1.\n",
    "\n",
    "Exemple :\n",
    "Si nous déclarerions un tableau numpy S tel que S=np.zeros((16))\n",
    "Alors si le joueur est en position 3 => S[3]=1 \n",
    "\n",
    "S = [ 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2cacc8-eff4-4625-a785-bf897bd7e317",
   "metadata": {},
   "source": [
    "---\n",
    "## Réseau de neurones ( 4 pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd60144d-eb18-4170-ba30-c312ef44c76a",
   "metadata": {},
   "source": [
    "Pouvez vous donner une proposition de description pour ce réseaux en **expliquant pourquoi** :\n",
    "- nombre de neurones pour les couches d'entrée, cachée(s) et de sortie<br>\n",
    "- type de couche<br>\n",
    "\n",
    "Il y a plusieurs réponses possibles."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f38160fc-616f-4241-bc5d-2d267f5610e7",
   "metadata": {},
   "source": [
    "# VOTRE REPONSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3134929-d0c4-4539-a04c-1880f5092a5d",
   "metadata": {},
   "source": [
    "## Fonctions utiles (rien à coder ici)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27478b51-e6ad-472d-b60a-cdb2b9313de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe pour notre algorithme DQN\n",
    "class DQN:\n",
    "    def __init__(self, obs_size, action_size, gamma, learning_rate,layer1=16):\n",
    "        # Objet : initialisation de la classe\n",
    "        # Paramètres en entrée\n",
    "        #     Nombre de paramètres pour décrire l'état\n",
    "        #     Nombre d'actions\n",
    "        #.    Paramètre gamma : dépréciation futur\n",
    "        #     learning rate\n",
    "        \n",
    "        # dimension des états\n",
    "        self.obs_size = obs_size\n",
    "        \n",
    "        # nombre d'actions\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # gamma : dépréciation du futur\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # autres paramètres\n",
    "        self.update_freq = 100 # fréquence de copie des poids (1000)\n",
    "        \n",
    "        # autres structures\n",
    "        self.replay_buffer = deque(maxlen=5000) # enregistrements des résultats\n",
    "        \n",
    "        # Modèle de réseaux de neurones\n",
    "        self.main_network = self.build_model(layer1)\n",
    "        self.target_network = self.build_model(layer1)\n",
    "        \n",
    "        # recopie des poids pour avoir des réseaux à l'identique\n",
    "        self.target_network.set_weights(self.main_network.get_weights())\n",
    "        \n",
    "              \n",
    "    def build_model(self,layer1):\n",
    "        # input : size of the 2 layers\n",
    "        # output : model of neural network\n",
    "        model = Sequential()\n",
    "        model.add(Dense(layer1,input_shape=(self.obs_size,),activation='relu'))\n",
    "        model.add(Dense(self.action_size))\n",
    "        model.add(Activation('linear'))\n",
    "        # si keras 2.x\n",
    "        # model.compile(loss='mse',optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=self.learning_rate))\n",
    "        # si keras 3.x\n",
    "        model.compile(loss='mse',optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),jit_compile=True)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def store_transition(self,state,action,reward,next_state,done):\n",
    "        # input : \n",
    "        # output : nothing\n",
    "        self.replay_buffer.append((state,action,reward,next_state,done))\n",
    "        \n",
    "    def policy(self, state, epsilon):\n",
    "        # Policy : return action regarding the state according to the policy\n",
    "        r = np.random.uniform()\n",
    "        if r < epsilon:\n",
    "            action = np.random.randint(self.action_size)\n",
    "        else:\n",
    "            actions = self.main_network.predict_on_batch(state)\n",
    "            action = np.argmax(actions[0])\n",
    " \n",
    "        return action\n",
    "\n",
    "    @tf.function\n",
    "    def _train_step(self, states, Q_values):    \n",
    "            self.main_network.fit(states,Q_values,epochs=1,verbose=0)\n",
    "        \n",
    "    def train_model_fast(self,batch_size):\n",
    "         # Implémentation parallèle\n",
    "        minibatch = random.sample(self.replay_buffer,batch_size)\n",
    "        states = np.array([i[0] for i in minibatch])\n",
    "        actions = np.array([i[1] for i in minibatch])\n",
    "        rewards = np.array([i[2] for i in minibatch])\n",
    "        next_states = np.array([i[3] for i in minibatch])\n",
    "        dones = np.array([i[4] for i in minibatch])\n",
    "\n",
    "        # on supprime l'axe y (=1) de x,y,z\n",
    "        states = np.squeeze(states)\n",
    "        next_states = np.squeeze(next_states)\n",
    "\n",
    "        # 1-done : si done on ne prend en compte que reward\n",
    "        targets = rewards + self.gamma*(np.amax(self.target_network.predict_on_batch(next_states), axis=1))*(1-dones) \n",
    "        \n",
    "        targets_full = self.target_network.predict_on_batch(states) \n",
    "        ind = np.array([i for i in range(batch_size)])\n",
    "        targets_full[[ind], [actions]] = targets      \n",
    "        \n",
    "        #self.main_network.fit(states, targets_full, epochs=1, verbose=0)\n",
    "        self._train_step(states,targets_full)\n",
    "        \n",
    "    def update_weights(self):\n",
    "        self.target_network.set_weights(self.main_network.get_weights())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59693ec1-e611-4f5d-8ec2-624494de339c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d84cfb-48e4-438e-a653-57a465e71986",
   "metadata": {},
   "source": [
    "## Variante DQN (3 pts)\n",
    "\n",
    "Pouvez vous décrire une variante de DQN et son intêret en quelques mots ? "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5397198-336b-4b89-a7d6-8e0d11efaaf4",
   "metadata": {},
   "source": [
    "# VOTRE REPONSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007af7db-98cd-405b-bab8-5cd24900818f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e8b7d4-230b-40e2-bcf0-6589493cd92c",
   "metadata": {},
   "source": [
    "# Représentation de l'état (3 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92ff9dc-3ba9-4ee4-80eb-37f999105bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation des données pour compatabilité avec l'alimentation du réseau de neurones (1,s)`\n",
    "def trans_state(s):\n",
    "    # VOTRE CODE 3 pts\n",
    "    # en entrée nous avons un entier compris en 0 et 15 (bornes incluses)\n",
    "    # en sortie un numpy de dimension [ 1 , nombre obervation ] \n",
    "    # Le tableau aura la valeur zéro (0) à toutes les positions sauf pour l'indice correspondant à s ou nous souhaitons la valeur un (1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3c2b1d-cc3b-40c6-a2dd-154525c3ddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ne devrait pas générer d'erreur\n",
    "assert (trans_state(5) == np.array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])).all() == True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715f0f5a-e07d-4ef9-8d8b-a7588d7213c9",
   "metadata": {},
   "source": [
    "----\n",
    "## Lancons la simulation\n",
    "Réussite et vérification : 4 pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40544b87-cea1-49c8-b7aa-caecff66e480",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "batch_size = 10 # K\n",
    "gamma=0.99 \n",
    "\n",
    "# variation du epsilon de notre politique e-greedy\n",
    "epsilon_max=1.00\n",
    "epsilon_min=0.10\n",
    "epsilon_decay=(epsilon_max-epsilon_min)/(num_episodes/2)\n",
    "\n",
    "learning_rate=0.001 # 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a6730c-1bc9-4f15-932f-1a6da59271fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN(nb_obs,nb_actions,gamma,learning_rate,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7a5c03-286a-47d9-8361-3136ca70777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time=time.time()\n",
    "last_time=start_time\n",
    "train_time=0\n",
    "time_step = 0  # comptage du nombre total de mouvement\n",
    "histoReturn=[] # pour graphique sur historique récompense\n",
    "epsilon=epsilon_max\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    # somme de la récompense total pour une cycle\n",
    "    Return = 0\n",
    "    \n",
    "    # reset env et conversion state\n",
    "    state = trans_state(env.reset()[0])\n",
    "    done=False\n",
    "    truncated=False\n",
    "    episode_length=0\n",
    "    while not(done or truncated):\n",
    "    \n",
    "        time_step += 1\n",
    "        episode_length+=1\n",
    "\n",
    "        # copie des poids du réseau d'entrainement vers le réseau cible selon la fréquence choisie\n",
    "        if time_step % dqn.update_freq == 0:\n",
    "            dqn.update_weights()\n",
    "        \n",
    "        # sélection d'une action selon notre politique\n",
    "        action = dqn.policy(state,epsilon)\n",
    "\n",
    "        # jouer l'action\n",
    "        next_state, reward, done, truncated , _ = env.step(action)\n",
    "        next_state=trans_state(next_state) # reformatage\n",
    "        if truncated:\n",
    "            done=True \n",
    "      \n",
    "        # enregistrement pour replay\n",
    "        dqn.store_transition(state, action, reward, next_state, done)\n",
    "        \n",
    "        # et shift d'état\n",
    "        state = next_state\n",
    "        \n",
    "        # cumul du retour G\n",
    "        Return += reward\n",
    "\n",
    "        if episode_length==100: # nous limitons les épisodes à 100 mouvements\n",
    "            truncated=True\n",
    "            \n",
    "        # Done ?\n",
    "        if done or truncated:    \n",
    "            # on décremmente epsilon\n",
    "            epsilon -= epsilon_decay\n",
    "            epsilon = max(epsilon_min, epsilon)\n",
    "            histoReturn.append(Return)\n",
    "\n",
    "            # affichage du résultat du cyle\n",
    "            if (i>0 and i % 100 ==0) or i == (num_episodes-1):\n",
    "                print(f\"Episode {i}, Taux de victoire {np.round(np.sum(histoReturn)/(i+1)*100,0)}%,\\\n",
    "                 epsilon {np.round(epsilon,2)}, durée {int(time.time()-last_time)}, training time {int(train_time)}\")\n",
    "                last_time=time.time()\n",
    "                train_time=0\n",
    "            break\n",
    "            \n",
    "        # à partir de + batch-size on entraine le reseau\n",
    "        train_start=time.time()\n",
    "        if len(dqn.replay_buffer) > batch_size: #  and i%10 == 0:\n",
    "            dqn.train_model_fast(batch_size)\n",
    "        train_end=time.time()\n",
    "        train_time += train_end-train_start\n",
    "print(\"--------------------------------------------------\")\n",
    "print(f\"Durée de l'entrainement = {np.round(time.time()-start_time,0)} secondes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4263c24-ab38-4596-acf5-b720f65a466e",
   "metadata": {},
   "source": [
    "Vous devriez arriver à un taux de réussite de près de 60% à la fin de l'entrainement.\n",
    "\n",
    "Si réussite 2 pts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3b8049-ba4b-4224-a5fb-020c07884ae1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21809c6e-72d4-4378-9a8e-b0dcf44a9db6",
   "metadata": {},
   "source": [
    "Que recommande notre classe dqn en position 0 et 14 ? ( 1pts)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4df574e8-1f4d-4598-be49-73b2e1dd4d89",
   "metadata": {},
   "source": [
    "# VOTRE REPONSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb29ec87-c197-4bf5-b726-46e874f6d6f6",
   "metadata": {},
   "source": [
    "Est ce correct ? (1 pts)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d829e240-da96-4605-a3a4-d248866591d9",
   "metadata": {},
   "source": [
    "# VOTRE REPONSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6400190f-d14b-4eca-8d05-303063e47c2a",
   "metadata": {},
   "source": [
    "---\n",
    "# Comparons avec Q-Learning Classique (rien à coder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55367f5a-aecc-4b9b-bc9c-0615722c3959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def politique_egreedy(env,Q,s,epsilon):\n",
    "    # en entrée : env=environnement, Q fonctions action valeur, s = état courant, epsilon= pour espilon-greedy\n",
    "    r=np.random.uniform()\n",
    "    if r<epsilon:\n",
    "        return(env.action_space.sample())\n",
    "    else:\n",
    "        return(np.argmax(Q[s,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbd5f64-e9b6-46de-8e6d-ae739addef64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598e006-2aa8-4aad-921d-95e3786d9dbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Nombre d'essai\n",
    "max_iter=50000\n",
    "alpha=0.50   # learning rate\n",
    "gamma=0.99   # part du futur\n",
    "epsilon=1.0 # part de hasard\n",
    "epsilon_min=0.10\n",
    "epsilon_decay=(epsilon-epsilon_min)/(max_iter/2)\n",
    "Q=np.zeros((env.observation_space.n,env.action_space.n))\n",
    "\n",
    "# liste des valeurs pour créer un graphique \n",
    "histoReturn=[]\n",
    "histoLongueurEpisode=[]\n",
    "start_time=time.time()\n",
    "for i in range(max_iter):\n",
    "    S=env.reset()[0]\n",
    "\n",
    "    done=False\n",
    "    truncated=False\n",
    "    cumulR=0 # cumul des récompenses par épisode\n",
    "    cumulA=0 # cumul du nombre d'action = longueur des épisodes\n",
    "    \n",
    "    while not (done or truncated):\n",
    "        A=politique_egreedy(env,Q,S,epsilon)\n",
    "        S_ , R, done, truncated, _ = env.step(A)\n",
    "        A_=politique_egreedy(env,Q,S_,0) # ## epsilon = 0 => pas de hasard = greedy\n",
    "        \n",
    "        Q[S,A] = Q[S,A] + alpha*(R+gamma*Q[S_,A_]-Q[S,A])\n",
    "\n",
    "        cumulR+=R\n",
    "        cumulA+=1\n",
    "        S=S_\n",
    "\n",
    "    epsilon -= epsilon_decay\n",
    "    epsilon = max(epsilon, epsilon_min)\n",
    "    \n",
    "    histoReturn.append(cumulR)\n",
    "    histoLongueurEpisode.append(cumulA)\n",
    " \n",
    "    if i % (max_iter/10) ==0 or i == (max_iter-1):\n",
    "      print(f\"Episode {i}, Taux de victoire {np.round(np.sum(histoReturn)/(i+1)*100,0)}%,\\\n",
    "      epsilon {np.round(epsilon,2)}\")\n",
    "\n",
    "print(f\"Durée de l'entrainement = {np.round(time.time()-start_time,0)} secondes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c1a93e-a7c5-46c1-a45b-c0bf27d73e8b",
   "metadata": {},
   "source": [
    "Vous devriez arriver à un taux autour de 60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d6e640-50aa-4675-9b26-000de8de0763",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(2,1,sharex=True)\n",
    "\n",
    "ax1.set_title(\"Taux de réussite\")\n",
    "ax2.set_title(\"Longueur des épisodes\")\n",
    "\n",
    "# fig.title(\"Récompenses cumulées par épisode\")\n",
    "\n",
    "ax2.plot(histoLongueurEpisode)\n",
    "ax1.plot(np.cumsum(histoReturn)/np.arange(max_iter)*100)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0363d1ef-e6f7-45c9-acd7-eab205aef2d4",
   "metadata": {},
   "source": [
    "---\n",
    "# Comparaison (3 pts)\n",
    "\n",
    "Nous venons de tester un algortihme avec réseau de neurones et un algoritgme classique.\n",
    "\n",
    "En comparant le temps de traitement, puis le nombre de d'épisode nécessaire, quelles conclusions pouvez vous tirer ?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "add107e1-8b45-4772-afb1-f6b8e1a94075",
   "metadata": {},
   "source": [
    "# VOTRE REPONSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1562ac-9ccf-45e9-bb65-d7715f4a216e",
   "metadata": {},
   "source": [
    "---\n",
    "## DQN vs Policy Gradient ( 5pts)\n",
    "\n",
    "Aurions nous pu utiliser policy gradient pour résoudre cet environnement ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee20d00-56f7-4ade-9c54-d8a255ebbde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOTRE REPONSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd853af1-b5fb-4a9f-8a63-7693998f6903",
   "metadata": {},
   "source": [
    "Citer 3 différences entre les deux implémentations (DQN vs Policy Gradient) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424b228d-1ee1-4bac-afba-a1e0058a2f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOTRE REPONSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607d30fb-a75a-4c0c-b31f-e1437d23f1eb",
   "metadata": {},
   "source": [
    "---\n",
    "# N'oubliez pas de m'envoyer le notebook par MIO !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
