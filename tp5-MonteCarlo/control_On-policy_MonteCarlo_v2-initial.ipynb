{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo On policy\n",
    "## Sources \n",
    "- Reinforcement Learning : An Introduction, by Richard Sutton and Andrew Barto\n",
    "\n",
    "Toujours basé sur le Blackjack, nous allons rechercher une politique optimum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création de l'environnement Blackjack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionnaire pour stocker les Q values (action-valeur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = defaultdict(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionnaire pour le total des retours G poour chaque couple s,a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_return = defaultdict(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionnaire pour le cumul du nombre de passage pour tout les s,a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = defaultdict(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définissons une politique epsilon-greedy\n",
    "Paramètres d'entrés   \n",
    "    - Q fonction (s,a)   \n",
    "    - L'état courant   \n",
    "    - epsilon   \n",
    " \n",
    "Paramètre de sortie    \n",
    "    - a action choisie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state,Q,epsilon):\n",
    "\n",
    "    # tirer un nombre aléatoire entre 0 et 1\n",
    "    # si ce nombre est inférieur à epsilon, tirer une action au hasard\n",
    "    # Sinon en partant de Q, pour l'état state, choisir l'action qui est la plus rémunératrice\n",
    "    # 10-15 lignes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Générateur d'épisode\n",
    "Entrée :\n",
    "- Q fonction\n",
    "- espilon pour notre politique aléatoire\n",
    "\n",
    "Sortie :\n",
    "- notre épisode sous la forma s,a ,s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timesteps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(Q,epsilon):\n",
    "    \n",
    "    episode = []\n",
    "    \n",
    "    state = env.reset()[0]\n",
    "    \n",
    "    for t in range(num_timesteps):\n",
    "        \n",
    "        # Sélection d'une action en fonction de notre politique\n",
    "        action = epsilon_greedy_policy(state,Q,espilon)\n",
    "        \n",
    "        # envoie de l'action à l'environnement pour retour (s_, r, done)\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        # stockage dans la liste du triplet (état, action, récompense)\n",
    "        episode.append((state, action, reward))\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Calcul de la politique optimale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 50000\n",
    "espilon = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons initialiser une politique random au démarrage.      \n",
    "Selon le modèle GPI, Notre Q fonction va tendre vers l'optimale et donc notre politique également.<br>\n",
    "Pour simplifier et parce que l'intéret est limité dans le cas du blackjack, nous ferons abstraction de gamma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "for i in range(num_iterations):\n",
    "    \n",
    "    # on génére un épisode\n",
    "    episode = generate_episode(Q,espilon)\n",
    "    \n",
    "    # on stocker les pairs s,a de l'épisode\n",
    "    all_state_action_pairs = [(s, a) for (s,a,r) in episode]\n",
    "    \n",
    "    # on stocke les récompense\n",
    "    rewards = [r for (s,a,r) in episode]\n",
    "\n",
    "    # Pour chaque t de l'épisode \n",
    "    for t, (state, action, reward) in enumerate(episode):\n",
    "\n",
    "        # First visit : on ne prend en compte que le premier passage s,a\n",
    "        if not (state, action) in all_state_action_pairs[0:t]:\n",
    "            \n",
    "            # Calcul de G avec y = 1\n",
    "            R = # vode code\n",
    "            \n",
    "            # Cumul G\n",
    "            total_return[(state,action)] = # votre code\n",
    "            \n",
    "            # Comptage du nombre de passage\n",
    "            N[(state, action)] += 1\n",
    "\n",
    "            # Calcul de Q value (s,a) par la moyenne des G cumulés sur N\n",
    "            # Votre code : mettre à jour Q, ce qui mettrait à jour notre politique\n",
    "            # 1 ligne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stockage du résultat dans un Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Q.items(),columns=['state_action pair','value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons un Q optimal ou proche.   \n",
    "Notre politique est basée sur une approche gloutonne de Q donc elle même optimale !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Exemple\n",
    "J'ai en main 19 points (sans as) et le croupier 5 points visibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[ df['state_action pair'] == ((19,5,False),0) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[ df['state_action pair'] == ((19,5,False),1) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ma politique va donc choisir un stand (0) car la valeur de fonction d'action-état est la plus haute !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
