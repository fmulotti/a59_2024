{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c74f263-ff78-4fe7-b7da-800396652c3b",
   "metadata": {},
   "source": [
    "---\n",
    "# Programmation dynamique\n",
    "\n",
    "\n",
    "Fabrice Mulotti<br>\n",
    "\n",
    "v2 2023\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf16a8fb-cd3a-4b53-bff2-abe3b9a3a5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb77d95-fba9-42cd-8fb1-dc715521d2f9",
   "metadata": {},
   "source": [
    "---\n",
    "## Frozen Lake\n",
    "\n",
    "Découvrons notre environnement <br>\n",
    "<br>\n",
    "![ForzenLake](images/frozen_lake.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbf365b-8365-483c-9f43-e82eb4bf4105",
   "metadata": {},
   "source": [
    "https://gymnasium.farama.org/environments/toy_text/frozen_lake/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7e991f-ecec-4312-b19f-3085de1ee086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# déclaration de l'environnement\n",
    "env = env = gym.make('FrozenLake8x8-v1',is_slippery = False,map_name=\"4x4\", render_mode=\"ansi\") # ,render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c49269-53b4-4c76-a31e-db228acddafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage\n",
    "env.reset()\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e149943c-56c1-4c63-97ca-6ebb5dff23b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nombre d'états\n",
    "env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3068a2b9-d0ec-47a6-928d-6979ea1be401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nombre d'actions possibles\n",
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2201db5-cf31-4b14-8322-c679035b546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT=0\n",
    "DOWN=1\n",
    "RIGHT=2\n",
    "UP=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df46dbcb-eec8-47b0-8e4d-59cbd23aaf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tirage aléatoire de fonction\n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f9572a-9f2a-4e8d-b82c-3d42e357a0f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5db5e43-a138-44eb-b2aa-0f9a079eedd3",
   "metadata": {},
   "source": [
    "---\n",
    "## action\n",
    "\n",
    "https://gymnasium.farama.org/api/env/#gymnasium.Env.step\n",
    "<br>\n",
    "env.step retourne les infos suivantes :<br>\n",
    "- observation (s')<br>\n",
    "- reward (r)<br>\n",
    "- termination (bool)<br>\n",
    "- truncated (bool)<br>\n",
    "- info <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3f11bb-866b-4d7d-8832-99213d68b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "r=env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b65c35-8455-428b-b82c-4784289663b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e263ea-20f4-4a4f-857c-5b6931f30beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Récompense {r[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f527683d-b629-49ba-a13d-4b4ac60cf866",
   "metadata": {},
   "source": [
    "## Matrice de transition\n",
    "\n",
    "__env.P[etat][action] retourne :__<br>\n",
    "n fois :<br>\n",
    "Probabilité<br>\n",
    "s'<br>\n",
    "r<br>\n",
    "état final ? <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1498b2dc-1350-4f8c-ad51-28577c7ee169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de transition, exemple s=4\n",
    "env.unwrapped.P[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e890f4-c365-4279-afa2-2c28c1261b06",
   "metadata": {},
   "source": [
    "Si le sol n'est pas glissant : <br>\n",
    "1 action => 1 état suivant <br>\n",
    "\n",
    "Si le sol est glissant : <br>\n",
    "3 destinations possibles (33% de prob), dont une en terminaison <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c275aec8-6cd1-424e-88ae-e90a069e1c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# récompense\n",
    "env.unwrapped.P[4][RIGHT][0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c5f15c-7feb-48b3-a33d-d09eb5eb8d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prochain état \n",
    "env.unwrapped.P[4][RIGHT][0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff54236-fd50-4e0a-a575-6db80d2dc3b3",
   "metadata": {},
   "source": [
    "---\n",
    "## Test complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89074e4c-c3da-4b52-985f-360f2c67f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S: initial state\n",
    "# F: frozen lake\n",
    "# H: hole\n",
    "# G: the goal\n",
    "\n",
    "env.reset()\n",
    "fin=False\n",
    "print(env.render())\n",
    "c=0\n",
    "while not fin:\n",
    "    action=env.action_space.sample()\n",
    "    r=env.step(action)\n",
    "    print(f\"Action={action}, {r}\")\n",
    "    fin = r[2] or r[3]\n",
    "    time.sleep(0.5)\n",
    "    print(env.render())\n",
    "    c+=1\n",
    "    if c==10:\n",
    "        fin=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20beaf94-a65d-48c5-aff9-f492646cd27c",
   "metadata": {},
   "source": [
    "---\n",
    "# Itération sur politique\n",
    "\n",
    "![Politique](images/politique.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b991918-956d-47aa-b804-b4a24ca78279",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 0.005 # Notre limite de convergence\n",
    "gamma = 0.8 # dépréciation du futur\n",
    "\n",
    "V = np.zeros((env.observation_space.n)) # initialisation fonction de valeur\n",
    "Policy = np.zeros((env.observation_space.n)) # initialisation d'une politique\n",
    "\n",
    "loopCounter=0\n",
    "while True:\n",
    "    # Policy evaluation -----------------------------------\n",
    "    while True:\n",
    "        delta = 0\n",
    "        loopCounter+=1\n",
    "        for s in range(env.observation_space.n):\n",
    "            v = V[s]\n",
    "            action = # votre code , choisir une action en fonction de la politique\n",
    "            q=0\n",
    "            for destination in env.P[s][action]:\n",
    "                q+= # votre code, \n",
    "            V[s]=q\n",
    "            delta = max(delta,np.abs(v-V[s]))\n",
    "\n",
    "        if delta < theta:\n",
    "            break;\n",
    "\n",
    "    # Policy improvement --------------------------------\n",
    "    policy_stable=True\n",
    "    for s in range(env.observation_space.n):\n",
    "        old_action=Policy[s]\n",
    "        Q=[]\n",
    "        for a in range(env.action_space.n):\n",
    "            q=0\n",
    "            for destination in env.P[s][a]:\n",
    "                q+=#\n",
    "                # votre code\n",
    "                # evaluer q pour chaque action en fonction des destinations possibles\n",
    "            Q.append(q)\n",
    " \n",
    "        new_action=#votre choisir l action en fonction d'une politique gloutonne\n",
    "        if new_action!=old_action:\n",
    "            policy_stable=False\n",
    "            Policy[s]=new_action\n",
    "    if policy_stable==True:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b74aebe-a7c9-4fa0-a800-7492e8b2a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "loopCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de19d4e3-1c35-4bb5-b8df-29fa8ff78712",
   "metadata": {},
   "outputs": [],
   "source": [
    "Policy.reshape(4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb7e827-58b8-460e-8169-0aeb1be891f4",
   "metadata": {},
   "source": [
    "![ForzenLake](images/frozen_lake.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6df52ba-29af-412e-b5ce-5de420e2e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "V.reshape(4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba45972-cca4-4c33-aa6c-f1d673cae3d3",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "slippery = False , deterministe<br>\n",
    "slippery = True , stocastique, choix des actions évitant le risque<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f456d9d6-4ee8-435a-a72f-3d39c3ff5669",
   "metadata": {},
   "source": [
    "---\n",
    "# Itération sur valeurs\n",
    "\n",
    "![valeur](images/iteration_valeur.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc34da9f-4b5e-4d11-b5b9-daf02f3a5305",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 0.005 # Notre limite de convergence\n",
    "gamma = 0.8 # dépréciation du futur\n",
    "\n",
    "V = np.zeros((env.observation_space.n)) # initialisation fonction de valeur\n",
    "Policy = np.zeros((env.observation_space.n)) # initialisation d'une politique\n",
    "\n",
    "# Update value function -----------------------------------\n",
    "while True:\n",
    "        delta = 0\n",
    "        loopCounter+=1\n",
    "        # pour chaque etat\n",
    "            # pour chaque action possible\n",
    "                # pour toutes les destinations possibles\n",
    "                    # cumuler la recompense\n",
    "            # mettre a jour V\n",
    "\n",
    "            delta = max(delta,np.abs(v-V[s]))\n",
    "        print(delta)\n",
    "        if delta < theta:\n",
    "            break;\n",
    "\n",
    "            \n",
    "# Policy  --------------------------------\n",
    "for s in range(env.observation_space.n):\n",
    "    Q=[]\n",
    "    for a in range(env.action_space.n):\n",
    "        q=0\n",
    "        for destination in env.P[s][a]:\n",
    "            probabilite=destination[0]\n",
    "            s_prime=destination[1]\n",
    "            recompense=destination[2]\n",
    "            q+=probabilite*(recompense+gamma*V[s_prime])\n",
    "        Q.append(q)\n",
    "    Policy[s]=# votre code glouton \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
