{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "715353ef-d186-4963-89e0-1f237fe11e17",
   "metadata": {},
   "source": [
    "# Policy Gradient\n",
    "## LunarLander\n",
    "## Fin en 800 épisodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943c575c-ae33-4d63-a56d-4ac3b7053030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b1cc2d-fbc8-4f25-9394-bdea24726fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8eae5c-5219-4c01-b303-5afd4e8dc772",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradient():\n",
    "    def __init__(self,nb_obs,nb_action,learning_rate=0.001,gamma=0.99, layer1=64,layer2=32):\n",
    "        self.nb_obs=nb_obs\n",
    "        self.nb_action=nb_action\n",
    "        self.learning_rate=learning_rate\n",
    "        self.gamma=gamma\n",
    "        \n",
    "        # création du réseau\n",
    "        self.policy, self.predict = self.buildNN(layer1,layer2)\n",
    "        \n",
    "        # mémoire des transitions\n",
    "        self.histo_states = []\n",
    "        self.histo_actions =[]\n",
    "        self.histo_rewards = []\n",
    "        \n",
    "\n",
    "        \n",
    "    def buildNN(self,layer1,layer2):\n",
    "        \"\"\"\n",
    "        Objet : création du réseau neural\n",
    "        entrée : couches cachées 1 et 2\n",
    "        sortie : réseau pour entrainement et prédiction\n",
    "        \"\"\"\n",
    "        InputOBS    = Input(shape=(self.nb_obs,))\n",
    "        InputReward = Input(shape=[1])\n",
    "        dense1 = Dense(layer1,activation='relu')(InputOBS)\n",
    "        dense2 = Dense(layer2, activation='relu')(dense1)\n",
    "        dense3 = Dense(int((layer2+self.nb_action)/2),activation='relu')(dense2)\n",
    "        \n",
    "        proba = Dense(self.nb_action,activation='softmax')(dense3)\n",
    "        \n",
    "        def loss_function(y_true, y_pred):\n",
    "            \"\"\"\n",
    "            Objet : fonction de perte\n",
    "            Entrée : y réel et y prédit\n",
    "            Sortie : loss\n",
    "            \"\"\"\n",
    "            out = tf.keras.backend.clip(y_pred,1e-8,1-1e-8) # on élimine les valeurs extrèmes\n",
    "            loss_brut = y_true * tf.keras.backend.log(out)\n",
    "            loss_reward = -loss_brut * InputReward\n",
    "            print(\"Loss reward \",loss_reward)\n",
    "            return tf.keras.backend.sum(loss_reward)\n",
    "    \n",
    "        policy=Model(inputs=[InputOBS,InputReward], outputs=[proba])\n",
    "        policy.compile(optimizer=Adam(learning_rate=self.learning_rate),loss=loss_function)\n",
    "        predict=Model(inputs=[InputOBS], outputs=[proba])\n",
    "        \n",
    "        return policy,predict\n",
    "                       \n",
    "    def act(self,state):\n",
    "        \"\"\"\n",
    "        Objet : choisi une action en fonction d'un état et de la distribution des prob\n",
    "        Entrée : état\n",
    "        Sortie : action\n",
    "        \"\"\"\n",
    "        # print(\"State \",state,state.shape)\n",
    "        # print(\"State [0]\",state[0],state[0].shape)\n",
    "        # state=state[np.newaxis,:]\n",
    "        proba=self.predict.predict(state,batch_size=1)[0]\n",
    "        # print(\"Proba \",proba,type(proba),proba.shape)\n",
    "        return np.random.choice(self.nb_action,1,p=proba)[0]\n",
    "    \n",
    "    def discount_reward(self,histo_reward):\n",
    "        \"\"\"\n",
    "        Objet : renvoie le gain d'une trajectoire \n",
    "        Entrée : historique des récompenses pour chaque transition \n",
    "        Sortie : Gain avec dépréciation gamma\n",
    "        \"\"\"\n",
    "        # creation d un array a zero memes dimensions que rewards\n",
    "        discounted = np.zeros_like(histo_reward)\n",
    "        G=0\n",
    "        for i in reversed(range(0,len(histo_reward))):\n",
    "            G=histo_reward[i]+G*self.gamma\n",
    "            discounted[i]=G\n",
    "        return discounted\n",
    "    \n",
    "    def memory(self,state,action,reward):\n",
    "        \"\"\"\n",
    "        Objet : historisation des transitions\n",
    "        Entrée : état, action, récompense\n",
    "        Sortie : none\n",
    "        \"\"\"\n",
    "        self.histo_states.append(state[0])\n",
    "        self.histo_actions.append(action)\n",
    "        self.histo_rewards.append(reward)\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Objet : entrainement du model en fonction de la dernière trajectoire\n",
    "        \"\"\"\n",
    "        states_history = np.array(self.histo_states)\n",
    "        actions_history = np.array(self.histo_actions)\n",
    "        rewards_history = np.array(self.histo_rewards)\n",
    "\n",
    "        # transformation de l'historique des action en matrice 0/1\n",
    "        actions=np.zeros([len(actions_history),self.nb_action])\n",
    "        actions[ np.arange(len(actions_history)), actions_history] = 1\n",
    "\n",
    "        # calcul du Gain normalisé\n",
    "        discounted_reward = self.discount_reward(rewards_history)\n",
    "        discounted_reward -= np.mean(discounted_reward)\n",
    "        discounted_reward /= np.std(discounted_reward)\n",
    "        # print(discounted_reward.shape)\n",
    "        \n",
    "        # entrainement\n",
    "        self.policy.train_on_batch([ states_history, discounted_reward ] , actions)\n",
    "        self.histo_states = []\n",
    "        self.histo_actions =[]\n",
    "        self.histo_rewards = []\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db080259-69df-4a16-85b5-82912261d854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation des données pour compatabilité avec l'alimentation du réseau de neurones (1,s)`\n",
    "def trans_state(s):\n",
    "    return  np.reshape(s, [1, nb_obs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79def83-a5f1-429a-a7a9-c96083a884bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('LunarLander-v3')\n",
    "nb_obs=env.observation_space.shape[0]\n",
    "nb_action=env.action_space.n\n",
    "print(f\"Nombre de carateristiques des états : {nb_obs}, nombre d actions possibles : {nb_action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f20e069-063f-4739-baf0-cb1cf01102cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=PolicyGradient(nb_obs,nb_action, learning_rate=0.001, gamma=0.99,layer1=64,layer2=32 ) # 0.98 l2 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb786483-874d-499d-9f52-0dfaee19e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path=os.path.join(\"poids\",\"pp_lunar_save_weights.hp5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325e2de6-2a20-4be4-b42a-15766ae1f0f5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_episodes = 20000\n",
    "time_step = 0  # comptage du nombre total de mouvement\n",
    "histoReturn=[] # pour graphique sur historique récompense\n",
    "\n",
    "seuilWin=180\n",
    "win=False\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    # somme de la récompense total pour une cycle\n",
    "    Return = 0\n",
    "    \n",
    "    # reset env et conversion state\n",
    "    state = trans_state(env.reset()[0])\n",
    "    done=False\n",
    "    truncated=False\n",
    "    startTime=time.time()\n",
    "    moveCount=0\n",
    "    while not(done or truncated):\n",
    "        \n",
    "        # décommenter pour affichage\n",
    "        # env.render()\n",
    "    \n",
    "        time_step += 1\n",
    "        moveCount += 1\n",
    "                \n",
    "        # sélection d'une action selon notre politique\n",
    "        action = agent.act(state)\n",
    " \n",
    "        # jouer l'action\n",
    "        next_state, reward, done, truncated , _ = env.step(action)\n",
    "        next_state=trans_state(next_state) # reformatage\n",
    "\n",
    "        # on met en memoire\n",
    "        agent.memory(state,action,reward)\n",
    "        \n",
    "        # et shift d'état\n",
    "        state = trans_state(next_state)\n",
    "        \n",
    "        # cumul du retour G\n",
    "        Return += reward\n",
    "\n",
    "       \n",
    "        # Done ?\n",
    "        if done or truncated:\n",
    "            # affichage du résultat du cyle\n",
    "            duration=time.time()-startTime\n",
    "            print('Episode: ',i, ',' 'Return', np.round(Return,2),', durée ',np.round(duration,2),' seconde en ',moveCount, ' mouvements, end=',truncated)\n",
    "            # graf.add_record(i,Return,0,moveCount,duration)\n",
    "            if np.mean(histoReturn[-5:]) > seuilWin:\n",
    "                print(\"Yeah ! Gagné !\")\n",
    "                win=True\n",
    "                \n",
    "            histoReturn.append(Return)\n",
    "\n",
    "            agent.train()\n",
    "            \n",
    "            # Sauvegarde des poids tous les 50 cycles\n",
    "            #if i % 50 == 0:\n",
    "            #    agent.policy.save_weights(checkpoint_path)\n",
    "                \n",
    "            break\n",
    "    if win:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c1785b-cdff-4fbd-95c3-0618284a0867",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(histoReturn)\n",
    "plt.plot([np.mean(histoReturn[x:x+50]) for x in range(len(histoReturn)-50)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c399596f-f906-4d45-b5da-cdad7a304791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b2e99d7-e9a7-4df6-98f4-17c9e82e11a8",
   "metadata": {},
   "source": [
    "# Visualisons le résultat !\n",
    "\n",
    "Fonctionne uniquement si vous n'utilisez pas wsl/docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aadfa8-a2fa-42b5-b13a-3c96371afdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test=gym.make('LunarLander-v3',render_mode=\"human\")\n",
    "nb_obs=env_test.observation_space.shape[0]\n",
    "nb_action=env_test.action_space.n\n",
    "print(f\"Nombre de carateristiques des états : {nb_obs}, nombre d actions possibles : {nb_action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7596e027-67dd-4159-ae8a-a63b4f6ed8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = trans_state(env_test.reset()[0])\n",
    "done=False\n",
    "truncated=False\n",
    "while not(done or truncated):\n",
    "        \n",
    "        # décommenter pour affichage\n",
    "        # env.render()\n",
    "        action = agent.act(state)\n",
    " \n",
    "        # jouer l'action\n",
    "        next_state, reward, done, truncated , _ = env_test.step(action)\n",
    "        next_state=trans_state(next_state) # reformatage\n",
    "    \n",
    "        # et shift d'état\n",
    "        state = trans_state(next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0821b99d-6782-4f22-802a-759b9bebc5de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
