{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de6f9cc4-46f6-47c8-8680-e3982fee4474",
   "metadata": {},
   "source": [
    "# DQN \n",
    "## Cartpole\n",
    "\n",
    "Fabrice Mulotti\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ddb3de-19da-4e49-a52e-2e7bd972f225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam # si vous utilisz un mac M1/M2 utiliser legacy.optimizers\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d11a31-55e3-4f47-989e-7206db109cbb",
   "metadata": {},
   "source": [
    "Si vous avez erreur TypeError: unhashable type: 'list', c'est que votre version de python est trop ancienne (conseillee 3.11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084aea21-30fb-4b3f-b89b-004c5abf0c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a7419a-fda1-4a7d-9bdb-03f193d88644",
   "metadata": {},
   "source": [
    "---\n",
    "## Définition de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eede1d-4dd6-498f-b2c3-2ea15a288abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v1'\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(100)\n",
    "env.reset(seed=100)\n",
    "nb_actions = env.action_space.n # Nombre d'action\n",
    "nb_obs = env.observation_space.shape[0] # nombre de paramètre pour décrire l'environnement\n",
    "print(f\"Nombre d actions : {nb_actions}, Dimension de l env : {nb_obs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc064f29-c6b1-456b-86eb-4d43106ae655",
   "metadata": {},
   "source": [
    "# Remarques sur l'environnement\n",
    "\n",
    "2 actions possibles :<br>\n",
    "    poussée vers la gauche<br>\n",
    "    poussée vers la droite<br>\n",
    "    \n",
    "Observation :<br>\n",
    "    Valeurs **continues** !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3134929-d0c4-4539-a04c-1880f5092a5d",
   "metadata": {},
   "source": [
    "## Fonctions utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88caf25-665b-417f-b375-7f8165d1033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe pour notre algorithme DQN\n",
    "class DQN:\n",
    "    def __init__(self, obs_size, action_size, gamma, learning_rate):\n",
    "        # Objet : initialisation de la classe\n",
    "        # Paramètres en entrée\n",
    "        #     Nombre de paramètre pour décrire l'état\n",
    "        #     Nombre d'actions\n",
    "        #.    Paramètre gamma : dépréciation futur\n",
    "        #     learning rate\n",
    "        \n",
    "        # dimension des états\n",
    "        self.obs_size = obs_size\n",
    "        \n",
    "        # nombre d'actions\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # gamma : dépréciation du futur\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # autres paramètres\n",
    "        self.update_freq = 100 # fréquence de copie des poids (1000)\n",
    "        \n",
    "        # autres structures\n",
    "        self.replay_buffer = deque(maxlen=100000) # enregistrements des résultats\n",
    "        \n",
    "        # Modèle de réseaux de neurones\n",
    "        self.main_network = self.build_model(16,8)\n",
    "        self.target_network = self.build_model(16,8)\n",
    "        # recopie des poids pour avoir des réseaux à l'identique\n",
    "        self.target_network.set_weights(self.main_network.get_weights())\n",
    "        \n",
    "        self.debug = 0\n",
    "        \n",
    "    def build_model(self,layer1,layer2):\n",
    "        # input : size of the 2 layers\n",
    "        # output : model of neural network\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(self.obs_size,)))\n",
    "        model.add(Dense(layer1,activation='relu'))\n",
    "        model.add(Dense(layer2,activation='relu'))\n",
    "        model.add(Dense(self.action_size))\n",
    "        model.add(Activation('linear'))\n",
    "        model.compile(loss='mse',optimizer=Adam(learning_rate=self.learning_rate),jit_compile=True)\n",
    "        return model\n",
    "\n",
    "    def store_transition(self,state,action,reward,next_state,done):\n",
    "        # input : \n",
    "        # output : nothing\n",
    "        self.replay_buffer.append((state,action,reward,next_state,done))\n",
    "             \n",
    "    def policy(self, state, epsilon):\n",
    "        # Policy : return action regarding the state according to the policy\n",
    "        r = np.random.uniform()\n",
    "        if r < epsilon:\n",
    "            action = np.random.randint(self.action_size)\n",
    "        else:\n",
    "            # ############################################################################\n",
    "            actions= # Votre code : prédiction\n",
    "            action = # \n",
    " \n",
    "        return action\n",
    "    \n",
    "                       \n",
    "    def train_model(self,batch_size):\n",
    "        self.debug=0\n",
    "        # sélection d'un échantillon\n",
    "        minibatch = random.sample(self.replay_buffer,batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*minibatch))\n",
    "\n",
    "        states=states.squeeze()\n",
    "        next_states=next_states.squeeze()\n",
    "        \n",
    "        # On récupere les Q values des états rencontrés\n",
    "        Q_values = self.main_network.predict(states,verbose = 0)\n",
    "        \n",
    "        Updates = rewards + self.gamma * np.amax(self.target_network.predict(next_states,verbose=0),axis=1) * (1-dones)\n",
    "        \n",
    "        Q_values[ np.arange(len(Updates)),actions] = Updates\n",
    "\n",
    "        # ############################################################################\n",
    "        # Votre code : entrainement\n",
    "        # Astuce : vous pouvez mettre l'entrainement dans une fonction avec un décorateur tf.function\n",
    "    \n",
    "    def update_weights(self):\n",
    "        self.target_network.set_weights(self.main_network.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92ff9dc-3ba9-4ee4-80eb-37f999105bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation des données pour compatabilité avec l'alimentation du réseau de neurones (1,s)`\n",
    "def trans_state(s):\n",
    "    return  np.reshape(s, [1, nb_obs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40544b87-cea1-49c8-b7aa-caecff66e480",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 2000\n",
    "batch_size = 10 # K\n",
    "gamma=0.98\n",
    "\n",
    "# variation du epsilon de notre politique e-greedy\n",
    "epsilon_max=1.00\n",
    "epsilon_min=0.05\n",
    "epsilon_decay=0.992 \n",
    "\n",
    "learning_rate=0.0001 # 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a6730c-1bc9-4f15-932f-1a6da59271fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN(env.observation_space.shape[0],env.action_space.n,gamma,learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7a5c03-286a-47d9-8361-3136ca70777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = 0  # comptage du nombre total de mouvement\n",
    "histoReturn=[] # pour graphique sur historique récompense\n",
    "epsilon=epsilon_max\n",
    "checkpoint_path=\"cartpole-save.weights.h5\"\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    # somme de la récompense total pour une cycle\n",
    "    Return = 0\n",
    "    \n",
    "    # reset env et conversion state\n",
    "    state = trans_state(env.reset()[0])\n",
    "    done=False\n",
    "    truncated=False\n",
    "    while not(done or truncated):\n",
    "            \n",
    "        time_step += 1\n",
    "        \n",
    "        # copie des poids du réseau d'entrainement vers le réseau cible selon la fréquence choisie\n",
    "        if time_step % dqn.update_freq == 0:\n",
    "            dqn.update_weights()\n",
    "        \n",
    "        # sélection d'une action selon notre politique\n",
    "        # votre code\n",
    " \n",
    "        # jouer l'action\n",
    "        # ############################################################################\n",
    "        # Votre code - jouez l'action et récupërer les éléments\n",
    "        # Changer le format de l'état obtenu (fonction dispo)\n",
    "        # 2 lignes\n",
    "        \n",
    "        if truncated:\n",
    "            done=True \n",
    "\n",
    "        # enregistrement pour replay\n",
    "        dqn.store_transition(state, action, reward, next_state, done)\n",
    "        \n",
    "        # et shift d'état\n",
    "        state = next_state\n",
    "        \n",
    "        # cumul du retour G\n",
    "        Return += reward\n",
    "\n",
    "        # Done ?\n",
    "        if done or truncated:\n",
    "            # affichage du résultat du cyle\n",
    "            print('Episode: ',i, ',' 'Return', Return,', epsilon ',np.round(epsilon,2),', end=',truncated)\n",
    "            # ou\n",
    "            # mise à jour de notre graphique sur les gains tous les 10 cycles\n",
    "            #if i % 10 == 0:\n",
    "            #    ax.clear()\n",
    "            #    ax.plot(histoReturn)\n",
    "            #    display(fig)\n",
    "            #    clear_output(wait=True)\n",
    "    \n",
    "            # on décremmente epsilon\n",
    "            epsilon *= epsilon_decay\n",
    "            epsilon = max(epsilon_min, epsilon)\n",
    "            histoReturn.append(Return)\n",
    "            # Sauvegarde des poids tous les 50 cycles\n",
    "            if i % 50 == 0:\n",
    "                dqn.main_network.save_weights(checkpoint_path)\n",
    "            break\n",
    "            \n",
    "        # à partir de + batch-size on entraine le reseau\n",
    "        if len(dqn.replay_buffer) > batch_size:\n",
    "            # Votre code : entrainement du réseau\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6400190f-d14b-4eca-8d05-303063e47c2a",
   "metadata": {},
   "source": [
    "---\n",
    "# Visualisons le résultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec39232-d9d4-41ff-9e94-7bd7d42624bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v1'\n",
    "\n",
    "env = gym.make(ENV_NAME,render_mode='human')\n",
    "nb_actions = env.action_space.n # Nombre d'action\n",
    "nb_obs = env.observation_space.shape[0] # nombre de paramètre pour décrire l'environnement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a123316b-b843-4fda-be72-d12b468e090c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7367a47-86fa-4746-9f5a-ad57e9bad9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pas besoin d'exécuter cette séquence si vous lancez le test après l'entrainement\n",
    "dqn = DQN(env.observation_space.shape[0],env.action_space.n,0,0)\n",
    "dqn.main_network.load_weights(\"cartpole-save.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213f405d-4599-4834-b7f6-54a3d4773c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "state=trans_state(env.reset()[0])\n",
    "done = False\n",
    "truncated=False\n",
    "Gain=0\n",
    "while not (done or truncated):\n",
    "    action = dqn.policy(state,0)\n",
    "    next_state, reward, done, truncated , _ = env.step(action)\n",
    "    Gain += reward\n",
    "    state=trans_state(next_state)\n",
    "print(f\"Gain {Gain}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55367f5a-aecc-4b9b-bc9c-0615722c3959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbd5f64-e9b6-46de-8e6d-ae739addef64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
